{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'C:/Users/leemj/Desktop/텍스트마이닝1_실습자료'\n",
    "os.chdir(os.path.join(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('imdb_review_classification.csv',engine='python')\n",
    "train = data.loc[data['type']=='train',:]\n",
    "test = data.loc[data['type']=='test',:]\n",
    "train = train.drop(['type'],axis=1)\n",
    "test = test.drop(['type'],axis=1)\n",
    "train = train.reset_index(inplace=False,drop=True)\n",
    "test = test.reset_index(inplace=False,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review, 'html.parser').get_text()\n",
    "    review_text = re.sub('[^a-zA-Z]',' ',review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( review_to_wordlist(raw_sentence) )\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leemj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/50000 preprocessing\n",
      "10000/50000 preprocessing\n",
      "15000/50000 preprocessing\n",
      "20000/50000 preprocessing\n",
      "25000/50000 preprocessing\n",
      "30000/50000 preprocessing\n",
      "35000/50000 preprocessing\n",
      "40000/50000 preprocessing\n",
      "45000/50000 preprocessing\n",
      "50000/50000 preprocessing\n",
      "540589\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "i = 1\n",
    "total = train.shape[0] + test.shape[0]\n",
    "for review in train['review']:\n",
    "    if i % 5000 == 0:\n",
    "        print('{}/{} preprocessing'.format(i,total))\n",
    "    sentences += review_to_sentences(review,tokenizer,remove_stopwords=False)\n",
    "    i += 1\n",
    "for review in test['review']:\n",
    "    if i % 5000 == 0:\n",
    "        print('{}/{} preprocessing'.format(i,total))\n",
    "    sentences += review_to_sentences(review,tokenizer,remove_stopwords=False)\n",
    "    i += 1\n",
    "print(len(sentences))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300 # 문자 벡터 차원 수\n",
    "min_word_count = 40 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context = 10 # 문자열 창 크기\n",
    "downsampling = 1e-3 # 문자 빈도수 Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 22:13:06,936 : INFO: collecting all words and their counts\n",
      "2019-06-27 22:13:06,938 : INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-27 22:13:07,012 : INFO: PROGRESS: at sentence #10000, processed 208074 words, keeping 10464 word types\n",
      "2019-06-27 22:13:07,123 : INFO: PROGRESS: at sentence #20000, processed 416297 words, keeping 14566 word types\n",
      "2019-06-27 22:13:07,192 : INFO: PROGRESS: at sentence #30000, processed 624721 words, keeping 17731 word types\n",
      "2019-06-27 22:13:07,287 : INFO: PROGRESS: at sentence #40000, processed 828515 words, keeping 20402 word types\n",
      "2019-06-27 22:13:07,361 : INFO: PROGRESS: at sentence #50000, processed 1041534 words, keeping 22657 word types\n",
      "2019-06-27 22:13:07,417 : INFO: PROGRESS: at sentence #60000, processed 1249208 words, keeping 24380 word types\n",
      "2019-06-27 22:13:07,472 : INFO: PROGRESS: at sentence #70000, processed 1464197 words, keeping 26331 word types\n",
      "2019-06-27 22:13:07,551 : INFO: PROGRESS: at sentence #80000, processed 1669385 words, keeping 28021 word types\n",
      "2019-06-27 22:13:07,606 : INFO: PROGRESS: at sentence #90000, processed 1886706 words, keeping 29637 word types\n",
      "2019-06-27 22:13:07,655 : INFO: PROGRESS: at sentence #100000, processed 2092083 words, keeping 30946 word types\n",
      "2019-06-27 22:13:07,705 : INFO: PROGRESS: at sentence #110000, processed 2297758 words, keeping 32350 word types\n",
      "2019-06-27 22:13:07,758 : INFO: PROGRESS: at sentence #120000, processed 2509462 words, keeping 33422 word types\n",
      "2019-06-27 22:13:07,845 : INFO: PROGRESS: at sentence #130000, processed 2719348 words, keeping 34642 word types\n",
      "2019-06-27 22:13:07,930 : INFO: PROGRESS: at sentence #140000, processed 2931729 words, keeping 35796 word types\n",
      "2019-06-27 22:13:08,019 : INFO: PROGRESS: at sentence #150000, processed 3156199 words, keeping 37022 word types\n",
      "2019-06-27 22:13:08,072 : INFO: PROGRESS: at sentence #160000, processed 3387724 words, keeping 38344 word types\n",
      "2019-06-27 22:13:08,126 : INFO: PROGRESS: at sentence #170000, processed 3613809 words, keeping 39443 word types\n",
      "2019-06-27 22:13:08,175 : INFO: PROGRESS: at sentence #180000, processed 3825874 words, keeping 40612 word types\n",
      "2019-06-27 22:13:08,231 : INFO: PROGRESS: at sentence #190000, processed 4049431 words, keeping 41624 word types\n",
      "2019-06-27 22:13:08,285 : INFO: PROGRESS: at sentence #200000, processed 4277148 words, keeping 42839 word types\n",
      "2019-06-27 22:13:08,340 : INFO: PROGRESS: at sentence #210000, processed 4505739 words, keeping 44014 word types\n",
      "2019-06-27 22:13:08,399 : INFO: PROGRESS: at sentence #220000, processed 4734325 words, keeping 45055 word types\n",
      "2019-06-27 22:13:08,452 : INFO: PROGRESS: at sentence #230000, processed 4953683 words, keeping 45961 word types\n",
      "2019-06-27 22:13:08,505 : INFO: PROGRESS: at sentence #240000, processed 5182858 words, keeping 46902 word types\n",
      "2019-06-27 22:13:08,570 : INFO: PROGRESS: at sentence #250000, processed 5404182 words, keeping 47857 word types\n",
      "2019-06-27 22:13:08,621 : INFO: PROGRESS: at sentence #260000, processed 5632766 words, keeping 48796 word types\n",
      "2019-06-27 22:13:08,672 : INFO: PROGRESS: at sentence #270000, processed 5856970 words, keeping 49708 word types\n",
      "2019-06-27 22:13:08,722 : INFO: PROGRESS: at sentence #280000, processed 6065366 words, keeping 50496 word types\n",
      "2019-06-27 22:13:08,771 : INFO: PROGRESS: at sentence #290000, processed 6276910 words, keeping 51387 word types\n",
      "2019-06-27 22:13:08,818 : INFO: PROGRESS: at sentence #300000, processed 6484666 words, keeping 52099 word types\n",
      "2019-06-27 22:13:08,867 : INFO: PROGRESS: at sentence #310000, processed 6694584 words, keeping 52912 word types\n",
      "2019-06-27 22:13:08,917 : INFO: PROGRESS: at sentence #320000, processed 6906466 words, keeping 53733 word types\n",
      "2019-06-27 22:13:08,965 : INFO: PROGRESS: at sentence #330000, processed 7112903 words, keeping 54477 word types\n",
      "2019-06-27 22:13:09,017 : INFO: PROGRESS: at sentence #340000, processed 7322379 words, keeping 55219 word types\n",
      "2019-06-27 22:13:09,065 : INFO: PROGRESS: at sentence #350000, processed 7530074 words, keeping 55949 word types\n",
      "2019-06-27 22:13:09,114 : INFO: PROGRESS: at sentence #360000, processed 7741268 words, keeping 56667 word types\n",
      "2019-06-27 22:13:09,162 : INFO: PROGRESS: at sentence #370000, processed 7946143 words, keeping 57409 word types\n",
      "2019-06-27 22:13:09,212 : INFO: PROGRESS: at sentence #380000, processed 8154820 words, keeping 58186 word types\n",
      "2019-06-27 22:13:09,263 : INFO: PROGRESS: at sentence #390000, processed 8370503 words, keeping 58961 word types\n",
      "2019-06-27 22:13:09,312 : INFO: PROGRESS: at sentence #400000, processed 8583725 words, keeping 59706 word types\n",
      "2019-06-27 22:13:09,362 : INFO: PROGRESS: at sentence #410000, processed 8796445 words, keeping 60430 word types\n",
      "2019-06-27 22:13:09,413 : INFO: PROGRESS: at sentence #420000, processed 9021444 words, keeping 61288 word types\n",
      "2019-06-27 22:13:09,466 : INFO: PROGRESS: at sentence #430000, processed 9242277 words, keeping 62092 word types\n",
      "2019-06-27 22:13:09,517 : INFO: PROGRESS: at sentence #440000, processed 9460172 words, keeping 62891 word types\n",
      "2019-06-27 22:13:09,585 : INFO: PROGRESS: at sentence #450000, processed 9688055 words, keeping 63667 word types\n",
      "2019-06-27 22:13:09,637 : INFO: PROGRESS: at sentence #460000, processed 9907412 words, keeping 64405 word types\n",
      "2019-06-27 22:13:09,690 : INFO: PROGRESS: at sentence #470000, processed 10129156 words, keeping 65145 word types\n",
      "2019-06-27 22:13:09,761 : INFO: PROGRESS: at sentence #480000, processed 10350877 words, keeping 65784 word types\n",
      "2019-06-27 22:13:09,814 : INFO: PROGRESS: at sentence #490000, processed 10577382 words, keeping 66521 word types\n",
      "2019-06-27 22:13:09,866 : INFO: PROGRESS: at sentence #500000, processed 10802611 words, keeping 67335 word types\n",
      "2019-06-27 22:13:09,916 : INFO: PROGRESS: at sentence #510000, processed 11019206 words, keeping 68008 word types\n",
      "2019-06-27 22:13:09,968 : INFO: PROGRESS: at sentence #520000, processed 11241364 words, keeping 68680 word types\n",
      "2019-06-27 22:13:10,021 : INFO: PROGRESS: at sentence #530000, processed 11466454 words, keeping 69431 word types\n",
      "2019-06-27 22:13:10,073 : INFO: PROGRESS: at sentence #540000, processed 11690866 words, keeping 70149 word types\n",
      "2019-06-27 22:13:10,079 : INFO: collected 70181 word types from a corpus of 11704946 raw words and 540589 sentences\n",
      "2019-06-27 22:13:10,079 : INFO: Loading a fresh vocabulary\n",
      "2019-06-27 22:13:10,130 : INFO: effective_min_count=40 retains 9558 unique words (13% of original 70181, drops 60623)\n",
      "2019-06-27 22:13:10,131 : INFO: effective_min_count=40 leaves 11396030 word corpus (97% of original 11704946, drops 308916)\n",
      "2019-06-27 22:13:10,166 : INFO: deleting the raw counts dictionary of 70181 items\n",
      "2019-06-27 22:13:10,170 : INFO: sample=0.001 downsamples 51 most-common words\n",
      "2019-06-27 22:13:10,171 : INFO: downsampling leaves estimated 8380139 word corpus (73.5% of prior 11396030)\n",
      "2019-06-27 22:13:10,206 : INFO: estimated required memory for 9558 words and 300 dimensions: 27718200 bytes\n",
      "2019-06-27 22:13:10,207 : INFO: resetting layer weights\n",
      "2019-06-27 22:13:10,372 : INFO: training model with 4 workers on 9558 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-06-27 22:13:30,838 : INFO: EPOCH 1 - PROGRESS: at 0.09% examples, 348 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:13:31,880 : INFO: EPOCH 1 - PROGRESS: at 0.35% examples, 1340 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:13:48,097 : INFO: EPOCH 1 - PROGRESS: at 0.45% examples, 954 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:13:49,224 : INFO: EPOCH 1 - PROGRESS: at 0.62% examples, 1298 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-27 22:14:05,505 : INFO: EPOCH 1 - PROGRESS: at 0.79% examples, 1176 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:14:07,227 : INFO: EPOCH 1 - PROGRESS: at 0.97% examples, 1392 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:14:24,325 : INFO: EPOCH 1 - PROGRESS: at 1.16% examples, 1265 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:14:26,015 : INFO: EPOCH 1 - PROGRESS: at 1.32% examples, 1427 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:14:42,630 : INFO: EPOCH 1 - PROGRESS: at 1.51% examples, 1325 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 22:14:44,872 : INFO: EPOCH 1 - PROGRESS: at 1.68% examples, 1444 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:14:59,837 : INFO: EPOCH 1 - PROGRESS: at 1.86% examples, 1378 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:15:02,158 : INFO: EPOCH 1 - PROGRESS: at 2.05% examples, 1475 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:15:17,379 : INFO: EPOCH 1 - PROGRESS: at 2.23% examples, 1411 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-27 22:15:20,409 : INFO: EPOCH 1 - PROGRESS: at 2.42% examples, 1488 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:15:34,551 : INFO: EPOCH 1 - PROGRESS: at 2.59% examples, 1443 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-27 22:15:37,808 : INFO: EPOCH 1 - PROGRESS: at 2.76% examples, 1509 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:15:51,783 : INFO: EPOCH 1 - PROGRESS: at 2.94% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:15:55,140 : INFO: EPOCH 1 - PROGRESS: at 3.11% examples, 1523 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:16:10,381 : INFO: EPOCH 1 - PROGRESS: at 3.30% examples, 1475 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:16:14,612 : INFO: EPOCH 1 - PROGRESS: at 3.45% examples, 1519 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 22:16:28,900 : INFO: EPOCH 1 - PROGRESS: at 3.63% examples, 1482 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6d36b7007d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                           \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_word_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                           \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                           sample=downsampling)\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mj\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s: %(message)s',\n",
    "                   level=logging.INFO)\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)# 필요없는 메모리 unload\n",
    "model_name = '300features_40minwords_10text'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['film'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match('man woman child kitchen'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE를 통해 시각화\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.models as g\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "model_name = '300features_40minwords_10text'\n",
    "model = g.Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "print(len(X))\n",
    "print(X[0][:10])\n",
    "tsne = TSNE(n_components=2)\n",
    "# 100개 단어만 시각화\n",
    "X_tsne = tsne.fit_transform(X[:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_tsne,index=vocab[:100],columns=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(40,40)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(df['x'],df['y'])\n",
    "\n",
    "for word,pos in df.iterrows():\n",
    "    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
